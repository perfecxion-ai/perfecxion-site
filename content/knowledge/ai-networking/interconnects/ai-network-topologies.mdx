---
title: 'AI Network Topologies: Architecture Patterns for Scale'
description: >-
  Analysis of network topologies optimized for AI workloads including fat-tree,
  dragonfly, and rail-optimized designs.
category: ai-networking
subcategory: networking
domain: ai-networking
format: article
date: '2025-08-18'
author: perfecXion AI Team
difficulty: advanced
readTime: 13 min read
tags:
  - network-topology
  - architecture
  - fat-tree
  - dragonfly
  - scale-out
  - AI-clusters
  - AI Networking
  - Networking
toc: true
featured: true
excerpt: >-
  Architecting Scalable AI Clusters: A Comparative Analysis of Fat-Tree,
  Dragonfly, and Clos Network Topologies

  Executive Summary

  Your AI models doubled in size. Your network didn't. Large-scale training now
  waits more on data movement than compute power. The network that used to be
  plumbing became yo...
status: published
---

# AI Network Topologies: Architecture Patterns for Scale

Architecting Scalable AI Clusters: A Comparative Analysis of Fat-Tree, Dragonfly, and Clos Network Topologies

Executive Summary

Your AI models doubled in size. Your network didn't.

Large-scale training now waits more on data movement than compute power. The network that used to be plumbing became your bottleneck. You need to choose between Fat-Tree (predictable but expensive), Dragonfly (cost-optimized but complex), or their modern variants. This choice determines whether your cluster scales or stalls.

Here's the reality check: Fat-Tree works beautifully up to 8,000 GPUs using a 2-tier leaf-spine design. Equal-Cost Multipath (ECMP) routing keeps it simple. Performance stays predictable. Your ops team already knows how to run it. NVIDIA's "rail-optimized" variant pushes density even higher, though you'll need serious cooling to handle the heat.

But scale past 8,000 GPUs and Fat-Tree's economics collapse. A 3-tier design needs so many optical cables that cabling costs more than compute. We're talking explosive growth—the number of expensive optical links grows faster than your GPU count.

Enter Dragonfly. It groups switches into dense, electrically-connected clusters, then connects these groups with minimal optical links. For systems over 16,000 nodes, Dragonfly cuts network costs by 52% compared to Fat-Tree. That's not optimization—that's survival at hyperscale.

The catch? Dragonfly used to choke on adversarial traffic patterns. HPE Slingshot solved this. Hardware-accelerated adaptive routing and congestion management turned Dragonfly from theory into reality. Frontier, Aurora, LUMI—the world's fastest supercomputers run Slingshot. They deliver consistent performance across hundreds of thousands of endpoints.

Your decision framework is clear. Under 8,000 GPUs? Fat-Tree wins on simplicity. Need extreme density? Rail-optimized Fat-Tree, if you can handle the cooling. Building for 16,000+ GPUs? Dragonfly with intelligent routing becomes mandatory. Fat-Tree's predictability matters less when you can't afford to build it.

Section 1: The Unique Networking Demands of Large-Scale AI Workloads

AI workloads break traditional network assumptions. General-purpose computing spreads traffic randomly. AI training creates structured, synchronous patterns that hammer specific paths. Miss these patterns when designing your network and you'll watch millions in GPUs sit idle.

Cost compounds the problem. Scale to tens of thousands of accelerators and your interconnect budget rivals your compute budget. Optical transceivers and cables dominate that cost. Choose the wrong topology and networking eats your entire infrastructure budget.

1.1 Characterizing AI Communication Patterns
Deep learning parallelization happens three ways. Each creates distinct network traffic.

Data Parallelism splits training data across workers. Each GPU holds the complete model but processes different data. After computing gradients, all workers synchronize using All-Reduce operations. This pattern dominates most large-scale training.

Operator/Tensor Parallelism splits individual layers across devices when models exceed single-GPU memory. Matrix multiplications get distributed. GPUs exchange intermediate results constantly—high-bandwidth, low-latency, usually within small groups.

Pipeline Parallelism chains model layers across GPU sets. Data flows through stages like an assembly line. Each stage talks primarily to its neighbors, creating producer-consumer patterns.

Notice what's missing? Random, uniform traffic. AI creates structured communication—toroids, meshes, rings. Building a network for full any-to-any bandwidth wastes money on paths that rarely get used. Meanwhile, the paths that do get used might be underprovisioned. This mismatch drove the shift to co-designed networks where topology matches workload.

1.2 Defining Key Metrics Beyond Bisection Bandwidth
Bisection bandwidth—the minimum bandwidth between network halves—tells you worst-case capacity. But AI training cares about different metrics.

All-Reduce Bandwidth directly predicts training speed. Data-parallel jobs spend significant time in All-Reduce. Optimize for this specific collective and you beat generic networks, even with lower theoretical bisection bandwidth.

Tail Latency kills synchronous training. Your cluster waits for the slowest worker. Average latency doesn't matter—the 99.9th percentile does. One delayed packet stalls thousands of GPUs. Networks must deliver consistent, predictable latency under load.

Congestion Resilience keeps multi-tenant clusters functional. Production clusters run multiple jobs simultaneously. One job's traffic spike shouldn't destroy another's performance. Good networks isolate congestion, protecting well-behaved flows from misbehaving neighbors.

1.3 The Economic Imperative: The Disproportionate Cost of Optical Interconnects at Scale
Money drives modern network design more than technology. Switches, NICs, cables—they all add up. But optical components dominate the bill.

Short links within racks use cheap Direct Attach Copper (DAC) cables. Long links between racks need expensive optical transceivers and fiber. The difference? 10-50x in cost, plus higher power consumption.

Fat-Tree architectures, with their massive spine layers, need thousands of long optical connections. Studies prove this: Dragonfly reduces costs by 52% versus Fat-Tree at 16,000+ nodes by minimizing optical links. Some specialized topologies like HammingMesh claim 8x cost reduction for AI workloads.

The message is brutal: optical links determine your budget at scale. Architectures that minimize long-distance connections win economically, even if they sacrifice some theoretical performance.

Section 2: The Fat-Tree (Folded Clos) Architecture: Predictability at a Price

Fat-Tree dominates today's data centers and AI clusters. Based on 1950s Bell Labs Clos networks, it delivers scalability, resilience, and predictable performance. Simple load-balancing makes it operationally friendly. But that predictability costs dearly at scale.

2.1 Architectural Deep Dive: 2-Tier (Leaf-Spine) and 3-Tier Non-Blocking Fabrics
Fat-Tree fixes traditional tree bottlenecks by adding bandwidth as you go up the hierarchy. "Fat" means thick pipes at the top.

The 2-tier leaf-spine design dominates:

- Leaf Layer: Top-of-Rack (ToR) switches connecting to servers
- Spine Layer: Core switches connecting to every leaf switch

Every server-to-server path crosses exactly three hops: leaf-spine-leaf. Uniform. Predictable. Simple.

Scalability depends on switch port count (P). For non-blocking 2-tier fabrics:
Maximum endpoints = P²/2

A 40-port switch supports 800 GPUs. A 128-port switch handles 8,192 GPUs.

Need more? Add a third tier, creating Fat-Trees of Fat-Trees. Maximum endpoints become:
Maximum endpoints = P³/4

That 64-port switch now supports 65,536 endpoints theoretically. But you'll need massive numbers of switches and optical links. The economics get ugly fast.

---
### Practical Example: Calculating Fat-Tree Scalability for AI Clusters

```python
# Calculate maximum endpoints for 2-tier and 3-tier Fat-Tree architectures
# Useful for planning AI cluster network scale

def fat_tree_endpoints(port_count, tiers=2):
    if tiers == 2:
        return (port_count ** 2) // 2
    elif tiers == 3:
        return (port_count ** 3) // 4
    else:
        raise ValueError("Only 2 or 3 tiers supported")

# Example usage:
print("2-tier, 40-port switch:", fat_tree_endpoints(40, tiers=2))
print("2-tier, 128-port switch:", fat_tree_endpoints(128, tiers=2))
print("3-tier, 64-port switch:", fat_tree_endpoints(64, tiers=3))
```

*This code helps architects quickly estimate the scalability limits of Fat-Tree topologies for AI clusters. Use it to guide hardware selection and topology design.*

---

2.2 Performance and Resilience: The Role of ECMP
Fat-Tree's magic comes from path redundancy. Every leaf connects to every spine, creating multiple equal-cost paths. ECMP routing exploits this.

ECMP hashes packet headers to pick paths, spreading traffic across all spines. Benefits are immediate:

- Load balancing prevents bottlenecks
- Failed components get routed around automatically
- No complex reconvergence protocols needed

This combination—uniform hops plus ECMP—gives Fat-Tree its signature: predictable, low-latency performance.

2.3 Physical Implementation and Failure Domain Analysis
Physical deployment is straightforward. Leaf switches sit in server racks. Spine switches cluster in end-of-row or middle-of-row locations. Optical cables connect every rack to the spine cluster.

Failure domains are well-defined:

- Server/NIC failure: One node affected
- Leaf switch failure: One rack offline (32-64 servers), but contained
- Spine switch failure: Entire fabric loses capacity proportionally (1/16th for 16 spines)
- Link failure: Temporary disconnection until rerouting

The predictability helps operations. Failures either hit one rack or degrade everyone equally. Troubleshooting stays simple. No mysterious performance variations across the cluster.

2.4 Specialized Variant: The "Rail-Optimized" Multi-Plane Fat-Tree
NVIDIA's DGX servers with 8 GPUs created the rail-optimized design. Instead of connecting all 8 NICs to one switch, each NIC connects to a different switch. Eight independent network "rails" emerge.

Impact on density is dramatic:

- Traditional: 64-port switch supports 4 eight-GPU nodes (8 ports each)
- Rail-optimized: Same switch supports 32 nodes (1 port each)

Eight-fold improvement in switch utilization. Fewer switches, lower cost.

The price? Physical complexity. Those 8 switches must be co-located for copper connectivity. You get ultra-dense, ultra-hot racks demanding liquid cooling. It's a trade—switch savings for infrastructure complexity.

Section 3: The Dragonfly Topology: Engineering for Cost-Efficiency at Exascale

Dragonfly attacks Fat-Tree's weakness: optical cable explosion at scale. By reimagining hierarchy, it maximizes cheap electrical links within groups and minimizes expensive optical links between groups. For exascale systems, it's not optional—it's necessary.

3.1 Architectural Principles: Hierarchical Groups as High-Radix Virtual Routers
Dragonfly's innovation: the "group" concept. Networks split into groups, each containing compute nodes and switches.

Within groups: All-to-all electrical connections. Every switch connects to every other switch using short, cheap cables.

Between groups: Global all-to-all connections. Every group connects to every other group using optical links.

Groups act as massive virtual routers. Any packet crosses at most one expensive optical link:

- Intra-group traffic: Never leaves the group, uses only electrical links
- Inter-group traffic: Three hops—local hop, one global optical hop, remote hop

Network diameter stays at 3-5 hops even for hundreds of thousands of nodes. Fat-Tree can't match this at scale.

3.2 The Core Economic Driver: Minimizing Global Optical Links
Dragonfly's design philosophy: align topology with cable economics.

Intra-group links use cheap Direct Attach Copper because groups fit in adjacent racks. Inter-group links need expensive optics for distance. By concentrating connections within groups, Dragonfly slashes optical requirements.

The "links per arc" parameter lets you tune the trade-off. Want more global bandwidth? Add optical links. Need to cut costs? Reduce them. This tunability lets architects balance performance against budget.

Research proves the savings: 52% cost reduction versus Fat-Tree at 16,000+ nodes. That's the difference between feasible and impossible at hyperscale.

3.3 Physical Implementation: A Study of Cray XC/EX Wiring
Cray's supercomputers show Dragonfly in practice.

Cray EX with Slingshot uses 64-port switches forming groups of 1-32 switches. Port allocation:

- L0: Down to compute nodes
- L1: Intra-group electrical connections
- L2: Inter-group optical connections

A 32-switch group might use 16 ports for nodes, 32 for intra-group, 16 for global. Creates a 512-port virtual router. The tension: more group connectivity means fewer global ports, increasing blocking.

Earlier Cray XC systems used 48-port Aries routers with color-coded links:

- Green: Within-chassis connections
- Black: Within-group, between chassis
- Blue: Global optical connections

Physical engineering maximizes electrical connectivity locally, minimizes optical globally. That's the entire point.

3.4 The Dragonfly+ Evolution: Mitigating Adversarial Traffic
Classic Dragonfly had a weakness: adversarial traffic patterns. Many sources hitting one destination saturate that group's limited incoming links.

Dragonfly+ fixed this by using Fat-Tree within groups instead of all-to-all. Hybrid benefits:

- Keeps cost savings from minimal inter-group optics
- Adds Fat-Tree's superior load balancing internally
- ECMP routing prevents internal hotspots

This evolution made Dragonfly robust for unpredictable, multi-tenant workloads.

3.5 Failure Domain Analysis
Dragonfly failures are complex and non-uniform.

Intra-group failures force traffic onto non-minimal paths—leaving the group and returning just for local communication. Wastes global bandwidth.

Global link failures reduce bandwidth between specific groups. Adaptive routing compensates through intermediate groups, but adds latency and load.

Full group failures remove compute and routing capacity simultaneously. Impact varies by location—adjacent groups suffer more. Reasoning about failures is harder than Fat-Tree's simple degradation.

Section 4: Comparative Analysis: Cost, Performance, and Physical Constraints

Choosing topologies requires analyzing cost, performance, scalability, and operations. No universal winner exists—only trade-offs aligned with requirements.

4.1 Modeling Interconnect Cost: Optical Link and Switch Scaling
Cost differentiates topologies dramatically at scale.

Fat-Tree costs scale super-linearly. Switches grow with O(N), but inter-switch optical links explode faster. The spine layer's optical requirements become prohibitive.

Dragonfly attacks this directly. Studies quantify 52% cost reduction at 16,000+ nodes. Specialized topologies like HammingMesh claim 8x cheaper for AI-specific patterns. Rail-optimized Fat-Tree achieves similar All-Reduce performance at 10% the cost.

Small clusters where 2-tier Fat-Tree suffices? Costs stay manageable. But 3-tier requirements? Dragonfly becomes financially mandatory.

4.2 Performance Under Load: Bisection Guarantees vs. Real-World Throughput
Performance trades predictability for efficiency.

Fat-Tree guarantees 100% non-blocking bisection bandwidth. Place workloads anywhere, get consistent performance. Expensive, potentially overprovisioned, but predictable.

Dragonfly provides variable performance based on traffic patterns. Local traffic excels. Global adversarial patterns can saturate links. Yet benchmarks show Dragonfly improving All-to-All completion by 56% versus Fat-Tree when properly managed.

The key: Dragonfly performance depends on routing intelligence. Dumb Dragonfly fails. Smart Dragonfly with dynamic management beats expensive Fat-Tree.

4.3 Scalability and Physical Footprint
Scaling approaches differ fundamentally.

Fat-Tree scaling is disruptive. Growing past 2-tier requires adding a third tier—massive recabling project. Discrete scaling steps based on switch radix make incremental growth difficult. Cable volume challenges physical infrastructure.

Dragonfly scales gracefully. Add new groups incrementally. Connect their global links without touching existing infrastructure. Perfect for phased deployments and evolving requirements.

4.4 Comparative Matrix of AI Cluster Network Topologies

Feature | Fat-Tree (3-Tier) | Rail-Optimized Fat-Tree | Dragonfly | Dragonfly+

Network Diameter | High (5) | Moderate (3-5) | Low (3) | Low (3-5)

Bisection Bandwidth | Guaranteed Non-Blocking | Tunable/Blocked by Rail | Tunable/Blocked Globally | Tunable/Blocked Globally

Cost Scaling (Switches) | High O(N) | Moderate | Low O(N) | Low O(N)

Cost Scaling (Optics) | Very High | High | Low (Minimized) | Low (Minimized)

Primary Failure Domain | Rack (Leaf) | Rack/Rail | Group/Global Link | Group/Global Link

Resilience Mechanism | ECMP | ECMP | Adaptive Routing | Adaptive + ECMP

Ideal Scale | <8k GPUs | Dense <16k GPUs | >16k GPUs | >16k GPUs

Key Limitation | Optical Cost & Power | Power/Cooling Density | Adversarial Traffic | Intra-Group Complexity

Section 5: Case Study: Learnings from the HPE Slingshot Interconnect

Theory meets reality in HPE Slingshot. Powering the world's fastest supercomputers, it proves how smart engineering overcomes Dragonfly's inherent challenges.

5.1 Slingshot as a Converged Ethernet-HPC Fabric
Slingshot chose Ethernet compatibility—a strategic convergence of HPC and data center worlds.

Being Ethernet-compliant eliminates network silos. HPC traffic and standard IP traffic share one fabric. No gateway bottlenecks. No dual networks. Storage, campus networks, and compute clusters communicate directly. Complexity drops, costs follow.

5.2 The Role of the High-Radix "Rosetta" ASIC
Slingshot's Rosetta ASIC delivers 64 ports at 200 Gb/s each. High radix enables massive scale.

More ports mean larger groups, more powerful virtual routers, lower diameter. Slingshot scales to 250,000 endpoints with just three switch hops. Fewer switches, less cable, lower power—radix drives efficiency.

5.3 Key Innovation: Hardware-Accelerated Congestion Management and Adaptive Routing
Slingshot's breakthrough: hardware-based traffic intelligence solving Dragonfly's congestion vulnerability.

Fine-Grained Adaptive Routing: Switches track real-time path loads. Routes adapt dynamically based on congestion, not static hashes. Traffic flows around hotspots automatically.

Advanced Congestion Management: Switches identify congestion-causing flows specifically. They signal sources to throttle problem flows only. Other traffic continues unaffected. Victim flows don't suffer for others' misbehavior.

Hardware implementation is crucial. Microsecond response times, line-rate decisions. Software can't match this. Hardware-defined intelligence makes Dragonfly work at scale.

5.4 Analysis of Exascale Deployments (Frontier, Aurora, LUMI)
Real-world results from Frontier, Aurora, and LUMI prove Slingshot's effectiveness.

Comparative benchmarks against InfiniBand-based Leonardo are revealing. InfiniBand showed lower point-to-point latency (1.02 µs vs 3.66 µs). But scale changes everything.

On Slingshot, moving from same-switch to different-group GPUs reduced performance by just 1%. Leonardo suffered 17% degradation with doubled latency. Network noise and variability plagued the InfiniBand system at scale.

The lesson? Microbenchmark advantages disappear under real load. Slingshot's adaptive mechanisms deliver consistency across massive, loaded systems. Managing traffic intelligently beats raw performance that degrades unpredictably.

Section 6: Synthesis and Architectural Recommendations

No universal best topology exists. Scale, density, budget, and operations determine the right choice.

6.1 Synthesizing the Trade-Offs: A Decision Framework for Topology Selection
Three fundamental trade-offs define your choice:

Predictability vs. Cost: Fat-Tree guarantees uniform performance at high optical cost. Dragonfly sacrifices guarantees for dramatic cost reduction through minimized optics.

Density vs. Infrastructure: Rail-optimized Fat-Tree maximizes density but demands extreme cooling. Standard designs need less infrastructure investment.

Static vs. Dynamic Resilience: Fat-Tree uses simple ECMP for predictable degradation. Dragonfly requires intelligent routing for robust operation.

6.2 Recommendation for Moderate Scale (<8,192 GPUs)
Stay with 2-tier Fat-Tree leaf-spine for clusters under 8,192 GPUs.

Why? It's mature, understood, simple. Modern 128-port switches support 8,192 endpoints without oversubscription. No 3-tier complexity. ECMP works well. Optical costs haven't exploded yet. Dragonfly's benefits don't justify its complexity at this scale.

6.3 Recommendation for High-Density Deployments
Choose rail-optimized Fat-Tree when footprint and switch minimization matter most.

This path demands commitment. Ultra-high-power racks. Direct liquid cooling. Substantial infrastructure investment beyond networking. Only pursue if you're equipped for the thermal and power challenges.

6.4 Recommendation for Hyperscale (>16,000 GPUs)
Dragonfly with intelligent routing (like Slingshot) becomes mandatory at hyperscale.

Fat-Tree's optical costs become prohibitive. Power and space requirements break budgets. Dragonfly's optical minimization directly addresses these constraints. Modern traffic management solved historical weaknesses. Proven performance in exascale systems provides confidence.

6.5 Future Outlook: The Impact of Co-Packaged Optics and Next-Generation ASICs
The landscape keeps evolving.

Next-generation interconnects like 400 Gb/s Slingshot double bandwidth while refining traffic management. Innovation continues relentlessly.

Co-Packaged Optics (CPO) could change the equation by slashing optical power and cost. Fat-Tree might become competitive at larger scales if optical penalties diminish.

Convergence accelerates as AI integrates with analytics and enterprise apps. Demand grows for unified fabrics handling all traffic types seamlessly.

The tension persists: guaranteed bandwidth versus cost optimization. Workload understanding and economic reality will continue driving architectural choices at ever-increasing scales.

Sources used in the report
