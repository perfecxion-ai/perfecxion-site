---
title: 'RoCEv2 and InfiniBand Security: Simulations and Analysis'
description: >-
  Security analysis and simulation results for RoCEv2 and InfiniBand protocols
  in AI environments
date: '2025-08-18'
category: infrastructure
tags:
  - RoCEv2
  - InfiniBand
  - Network Security
  - RDMA
  - Protocol Analysis
  - Simulation
difficulty: advanced
readTime: 15 min read
format: article
domain: infrastructure
topics:
  - RoCEv2
  - InfiniBand
  - Network Security
  - RDMA
  - Protocol Analysis
---
Your Guide to Simulation Frameworks for Security Research in RoCEv2 and InfiniBand Networks
Section 1: Architectural Foundations and Attack Surfaces of High-Performance Interconnects
High-performance computing (HPC) and artificial intelligence (AI) rely on two primary interconnect technologies: RDMA over Converged Ethernet version 2 (RoCEv2) and InfiniBand. Both provide the ultra-low latency and high bandwidth your demanding distributed workloads require through Remote Direct Memory Access (RDMA). RDMA lets one machine access another's memory without involving the remote machine's CPU. But their underlying architectural philosophies differ fundamentally. RoCEv2 leverages the ubiquitous and cost-effective Ethernet ecosystem, while InfiniBand provides a specialized, self-contained fabric. These architectural distinctions directly influence their performance characteristics, operational models, and, most critically for this analysis, their inherent security postures and attack surfaces. You need to understand these foundational differences to select appropriate simulation frameworks and design meaningful security experiments.

1.1 RoCEv2 (RDMA over Converged Ethernet): The Distributed Paradigm
RoCEv2 is an advanced network protocol that enables RDMA over standard Ethernet networks with IP-based routing capabilities. This design choice makes it attractive for large-scale data centers and cloud environments seeking to leverage existing infrastructure and expertise in Ethernet and IP networking.

1.1.1 Protocol Stack and Routing
Unlike its predecessor, RoCEv1, which was confined to a single Layer 2 broadcast domain, RoCEv2 operates over Layer 3 networks. It achieves this by encapsulating the InfiniBand transport protocol within a UDP/IP header. This allows RoCEv2 packets to be routed across subnets using standard IP routers, providing the scalability your modern data center fabrics require. This reliance on the standard TCP/IP protocol suite means that RoCEv2 inherits the security challenges associated with IP networking, even as it bypasses the kernel's TCP stack for data transfer. The UDP source port field is often used to carry an opaque flow identifier, which allows network devices to perform load balancing optimizations like Equal-Cost Multi-Path (ECMP) forwarding.

1.1.2 Lossless Ethernet Mechanisms
To support RDMA's stringent requirements, which assume a lossless transport, you must engineer RoCEv2 networks to prevent packet drops due to congestion. You achieve this through a combination of two key technologies operating at different layers of the network stack: Priority-based Flow Control (PFC) and Explicit Congestion Notification (ECN).

Priority-based Flow Control (PFC): Standardized as IEEE 802.1Qbb, PFC is a link-layer mechanism that prevents buffer overflow in switches. It operates on a per-priority basis, allowing a switch to send a "pause" frame to its upstream neighbor for a specific traffic class when the buffer for that class is nearing capacity. This pause command temporarily halts transmission for that priority, preventing packet loss. While effective, PFC can lead to significant performance issues, such as Head-of-Line (HoL) blocking and congestion spreading, if not managed carefully. This signaling mechanism is a primary target for denial-of-service attacks.

Explicit Congestion Notification (ECN): ECN is a Layer 3 mechanism that provides end-to-end congestion signaling without dropping packets. ECN-capable switches mark packets when their queue length exceeds a certain threshold. The receiver then relays this marking back to the original sender, signaling the onset of congestion and prompting the sender to reduce its transmission rate.

1.1.3 Congestion Control: Data Center Quantized Congestion Notification (DCQCN)
The de-facto standard congestion control algorithm for RoCEv2 is Data Center Quantized Congestion Notification (DCQCN). DCQCN is a rate-based, end-to-end scheme that combines concepts from QCN (IEEE 802.1Qau) and DCTCP. It defines how three entities behave:

Congestion Point (CP): The switch, which detects congestion (e.g., queue length exceeding a threshold) and marks passing packets with an ECN flag.

Notification Point (NP): The receiver's network interface card (NIC), which observes the ECN-marked packets and generates a Congestion Notification Packet (CNP) back to the sender.

Reaction Point (RP): The sender's NIC, which receives the CNP and reduces its sending rate according to a specific algorithm.

DCQCN's effectiveness hinges on endpoints reacting to congestion signals compliantly and promptly. Any deviation from this expected behavior, whether malicious or accidental, can undermine the entire network's stability, making the RP logic a key focus for your security analysis.

1.1.4 Distributed Management Model
A defining characteristic of RoCEv2 networks is their purely distributed nature. Unlike InfiniBand, there's no central management entity. The network consists of RoCEv2-capable NICs and switches that operate based on standard, distributed networking protocols. This architecture aligns with the operational model of most enterprise and cloud data centers but places the burden of configuration, management, and security on each individual component and the network fabric as a whole.

1.2 InfiniBand: The Centrally Managed Fabric
InfiniBand represents a different approach to high-performance networking, designed from the ground up as a specialized, switched fabric interconnect for HPC. It's not based on Ethernet or IP and uses its own unique protocol stack, requiring dedicated hardware components.

1.2.1 Protocol Stack and Architecture
An InfiniBand network is a self-contained "subnet" composed of key components: Host Channel Adapters (HCAs) in your servers, InfiniBand switches, and a centralized Subnet Manager (SM). The InfiniBand architecture defines the lower four layers of the OSI model but does not use Ethernet or IP for its core operations. This creates a more isolated and controlled network environment compared to RoCEv2.

1.2.2 Credit-Based Flow Control
InfiniBand achieves lossless operation through a fundamentally different mechanism than RoCEv2. Instead of relying on reactive pause frames like PFC, InfiniBand uses a proactive, link-level, credit-based flow control system. Before a sender transmits a packet, it must ensure that the receiver at the other end of the link has sufficient buffer space (credits) to accept it. The sending end will only initiate transmission after confirming the receiving end has enough credits. This credit-based handshake fundamentally prevents buffer overflows and packet loss at the link layer, making the fabric inherently lossless without the complexities and potential failure modes of PFC.

1.2.3 Centralized Management via the Subnet Manager (SM)
The most significant architectural differentiator of InfiniBand is its reliance on a centralized Subnet Manager (SM). The SM is a software entity, often running on a dedicated server or embedded within a switch, that's responsible for the overall management of the InfiniBand subnet. Its critical functions include:

Topology Discovery: The SM discovers all HCAs and switches in the fabric.

Path Calculation: It calculates and provisions the forwarding tables for every switch in the network. InfiniBand switches themselves don't run distributed routing protocols; they act as simple, fast forwarding engines based on the tables provided by the SM.

Partition and QoS Management: The SM is responsible for configuring network partitions (similar to VLANs) and Quality of Service (QoS) settings across the fabric.

This centralized management model simplifies your network devices but also introduces the SM as a single point of control and a potential single point of failure or attack.

1.3 Synthesizing the Attack Surfaces
The architectural divergence between RoCEv2 and InfiniBand results in fundamentally different security considerations and attack surfaces. Your decision to deploy one over the other isn't merely a matter of performance or cost but represents a choice between distinct security paradigms.

RoCEv2's integration with standard IP and Ethernet networking provides immense flexibility and scalability but simultaneously exposes it to a wider range of known attack vectors. Its reliance on cooperative, multi-layer mechanisms for losslessness—PFC at Layer 2 and ECN/DCQCN at Layer 3—creates a complex signaling environment that attackers can manipulate. Malicious actors can target these signaling protocols directly. For instance, a PFC pause frame storm attack exploits the link-layer flow control mechanism to create a denial-of-service condition. Similarly, an ECN manipulation attack involves a malicious endpoint ignoring congestion signals, thereby gaining an unfair share of bandwidth and destabilizing the network. These attacks target the very protocols that enable RoCEv2's performance.

In contrast, InfiniBand's closed, non-IP-based architecture provides a degree of intrinsic isolation from common network-layer attacks. Its proactive, credit-based flow control is a direct hardware handshake between adjacent link partners, making it far more robust against the type of signaling manipulation that can plague PFC. The attack surface shifts from the distributed data plane to the centralized control plane. The Subnet Manager becomes the most critical asset to protect; a compromised SM could potentially reroute traffic, violate network partitions, or bring down the entire fabric by distributing incorrect forwarding tables.

Consequently, if you're investigating network security in these environments, you must tailor your experiments to the appropriate architecture. Simulating a PFC pause frame attack is a highly relevant scenario for RoCEv2 but is architecturally irrelevant for a pure InfiniBand fabric. Conversely, modeling an attack on the Subnet Manager is a critical scenario for InfiniBand but has no equivalent in the distributed world of RoCEv2. This architectural dichotomy must guide your selection of simulation frameworks and the design of any security-related experiments.

Section 2: The Researcher's Arsenal: A Comparative Evaluation of Simulation and Emulation Frameworks
Selecting the appropriate tool is a critical first step in conducting meaningful research on high-performance interconnects' security and performance. The landscape of available tools spans a spectrum from high-fidelity, packet-level simulators that allow for deep protocol modification to realistic network emulators and vendor-specific digital twins that model system-level behavior. Each category of tool presents a unique set of capabilities and limitations, making the choice dependent on the specific research question being addressed.

2.1 High-Fidelity Packet-Level Simulation
Packet-level simulators provide the highest degree of flexibility and detail, allowing you to model and modify network protocols at the source-code level. They're indispensable for designing and evaluating novel attacks, defenses, and congestion control algorithms.

2.1.1 NS-3 (Network Simulator 3)
NS-3 is a discrete-event network simulator written in C++, renowned for its detailed protocol models, extensibility, and strong community support. For RoCEv2 research, NS-3 is arguably the most powerful and well-supported open-source tool available, primarily due to a specific, publicly available module.

The ns3-rdma Model: This third-party module is the cornerstone for your RoCEv2 research in NS-3. It provides a comprehensive implementation of the RoCEv2 protocol stack, including critical components for security and performance analysis:

Congestion Control: Full implementations of DCQCN and TIMELY congestion control algorithms.

Lossless Mechanisms: Models for both PFC and ECN, enabling the study of their interactions and vulnerabilities.

Hardware Modeling: A model of a Broadcom shared-buffer switch ASIC, which includes logic for buffer thresholding related to PFC triggering and ECN marking.

Limitations: The primary ns3-rdma repository is based on an older version of NS-3 (v3.17), which may require significant effort for you to port to more recent versions of the simulator. Furthermore, the implementation of the TIMELY congestion control algorithm resides on a separate code branch, making it difficult to simulate and directly compare DCQCN and TIMELY within the same experiment without merging the codebases. While this module provides excellent support for RoCEv2, there's no equivalently mature or well-maintained model for the InfiniBand architecture in the public domain, limiting NS-3's utility for your InfiniBand-specific research.

2.1.2 OMNeT++
OMNeT++ is another powerful, modular, and component-based C++ simulation framework widely used in academic and industrial research. Its graphical capabilities and extensive model libraries, such as the INET Framework, make it a versatile tool for network simulation.

RoCEv2 and InfiniBand Models: The support for RDMA interconnects in OMNeT++ stems largely from models contributed by vendors and academic projects. A notable Mellanox-contributed InfiniBand model provides a flit-level simulation of the data path in hosts and switches, making it suitable for performance analysis. Academic research has extended this InfiniBand model to implement PFC and RoCEv2 Congestion Management (RCM), demonstrating the framework's flexibility. The rationale for building a RoCEv2 model on top of an InfiniBand model is the architectural similarity in the upper layers and within the NICs themselves, which often share hardware/firmware components.

Limitations: While models exist, the OMNeT++ ecosystem for RDMA research appears more fragmented than that of NS-3. The available RoCEv2 models are the result of specific research projects and may not be as comprehensive or actively maintained as the ns3-rdma module. You may need to invest more effort in integrating or extending these separate components to build a complete simulation environment for modern security analysis.

2.1.3 Broadcom htsim
htsim is a high-performance, C++-based discrete event simulator developed with a specific purpose: the rapid examination of congestion control algorithm behavior. It's not a general-purpose network simulator but rather a specialized tool optimized for speed and focus.

Protocol Support: htsim includes simple models for a range of modern datacenter congestion control algorithms, including DCTCP, DCQCN, Swift, and HPCC. Crucially, it also features support for PFC and a basic RoCE model, making it relevant for the research context. Its primary advantage is its speed, which is reported to be much faster than older simulators like ns-2.

Limitations: The primary limitation of htsim is its narrow focus. It's designed for analyzing transport-level phenomena and may lack the detailed models of other network layers (e.g., complex routing, application-level logic) necessary for simulating system-wide, multi-faceted security attacks. Documentation is also noted to be sparse, which can increase the learning curve for new users. It's an excellent tool if you're focused purely on the performance of a new congestion control algorithm but may be insufficient for studying attacks that involve interactions across multiple protocol layers.

2.2 Network Emulation and Vendor Platforms
Emulation platforms offer a different approach by running real network software in a virtualized environment. This provides a higher degree of system-level realism at the cost of the protocol-level flexibility offered by simulators.

2.2.1 Mininet
Mininet is an instant virtual network emulator that creates a realistic network of hosts, links, and switches on a single machine using Linux network namespaces and process-based virtualization. It's the standard tool for prototyping and experimenting with Software-Defined Networking (SDN) and is tightly integrated with the P4 behavioral model software switch (BMv2).

Limitations for RDMA: Mininet's core functionality is built around standard TCP/IP networking and OpenFlow-based control. It has no native support for the specialized hardware offloads and protocols your RoCEv2 or InfiniBand systems require. While software implementations like Soft-RoCE exist, attempts to run them over a Mininet topology have been officially noted as unsupported by vendors and are unlikely to yield performance-accurate results. Therefore, Mininet is unsuitable for your high-fidelity RDMA performance or security analysis. However, its indispensable role in the P4 ecosystem makes it your primary choice for research into programmable data planes and, by extension, the security of In-band Network Telemetry (INT).

2.2.2 NVIDIA Air
NVIDIA Air is a commercial, cloud-based platform that provides a "digital twin" of a data center network. It allows users to create and interact with a virtualized replica of a real-world deployment, including NVIDIA Spectrum Ethernet switches (running actual Cumulus Linux or SONiC network operating systems), BlueField DPUs, and ConnectX NICs.

Use Case: NVIDIA Air is an exceptionally powerful tool for network operations, automation, and pre-deployment validation. Engineers can design complex leaf-spine topologies, test Ansible automation scripts, validate routing configurations, and demonstrate network behavior without needing access to physical hardware. It provides the highest possible fidelity in terms of how a real vendor software stack will behave.

Limitations for Security Research: The primary limitation of NVIDIA Air for fundamental security research is its closed-source, commercial nature. As an emulation platform running production software, it doesn't permit you to modify the internal logic of the protocols themselves. You cannot, for example, alter the DCQCN algorithm within the virtual NIC's firmware or inject a malformed PFC frame at the MAC layer. Its use is limited to launching attacks that can be generated through standard network traffic tools and APIs. It's therefore a platform for validating the system-level impact of known attack patterns, not for discovering or prototyping novel protocol-level vulnerabilities.

2.3 The Simulation vs. Emulation Trade-off for Security Research
The choice between these frameworks exposes a fundamental trade-off between protocol-level fidelity and system-level realism. This distinction is critical for your security research. To discover and model novel vulnerabilities that exploit the internal logic of a protocol, the complete control and transparency of a simulator is paramount. To test the impact of these vulnerabilities on a full, complex vendor software stack, the realism of an emulator is invaluable.

An attack like ECN manipulation, where a malicious node deliberately violates the DCQCN specification by not reducing its rate, requires direct modification of the congestion control algorithm's state machine. This is only feasible within a simulator like NS-3, which provides full source code access to its protocol models. An emulation platform like NVIDIA Air, which runs the actual, unmodified firmware on its virtual NICs, would not allow for such a modification.

Conversely, once you've designed and proven an attack in a simulator, you might want to understand how it affects a real network operating system with all its complexities. NVIDIA Air would be the ideal platform to test an attack that can be generated externally (e.g., a traffic pattern that triggers a known performance bug), providing a realistic assessment of its impact on system-wide behavior, logging, and management interfaces.

Mininet occupies a unique middle ground specifically for research on programmable data planes. It emulates the network topology, but the P4 software switch (BMv2) that runs within it is itself a simulator of a programmable ASIC. This powerful combination allows for the high-fidelity simulation of custom data plane logic (such as that used for In-band Network Telemetry) within a realistic, emulated network of hosts, making it the perfect tool for that specific domain. Your comprehensive research workflow might therefore involve using NS-3 to design and prototype a novel protocol exploit, followed by testing its system-level impact (if possible) in NVIDIA Air to validate its effects on a production-like environment.

Key Table: Comparative Analysis of Simulation and Emulation Frameworks
Criterion	NS-3	OMNeT++	Broadcom htsim	Mininet	NVIDIA Air
Primary Use Case	Detailed, packet-level network simulation	Modular, component-based network simulation	High-speed congestion control algorithm analysis	SDN/P4 network emulation	Data center digital twin and validation
RoCEv2 Support	
Excellent (via ns3-rdma module) 

Moderate (via academic extensions) 

Basic model for CC analysis 

None (Soft-RoCE unsupported) 

High-fidelity emulation of NVIDIA hardware 

InfiniBand Support	None (no mature public model)	
Good (Mellanox-contributed flit-level model) 

None	None	
High-fidelity emulation of NVIDIA hardware 

Congestion Control Models	
DCQCN, TIMELY 

RCM, IB CC 

DCQCN, Swift, HPCC, DCTCP 

N/A (TCP/IP focused)	Emulates hardware-native CC (e.g., DCQCN)
Telemetry (INT/P4) Support	None	None	None	
Excellent (via BMv2 software switch) 

Emulation of INT-capable hardware (e.g., Spectrum)
Extensibility for Security	High (full C++ source code modification)	High (full C++ source code modification)	Moderate (focused on CC logic)	High (for P4/SDN logic)	Low (closed, commercial platform)
Scalability	Moderate (can be resource-intensive)	Moderate	High (optimized for CC simulation)	High (for emulated nodes)	Very High (cloud-based, large topologies)
Learning Curve/Community	High / Excellent	High / Good	Moderate / Limited	Moderate / Excellent	Low / Vendor-supported
Section 3: Modeling Collective Communication: Simulating All-Reduce Workloads
To accurately assess the security and performance of interconnects for HPC and AI, it's not sufficient to model simple point-to-point traffic. The dominant communication patterns in these domains are collective operations, such as all-reduce, which involve complex, synchronized data exchanges among many nodes. The bursty and often conflicting traffic generated by these operations creates the network congestion that makes the system vulnerable to performance degradation and security attacks. Therefore, realistic workload modeling is a critical component of any meaningful simulation study.

3.1 Synthetic Workload Generation
A baseline approach to modeling collective communication is to create synthetic traffic patterns that approximate the behavior of an all-reduce operation using your simulator's standard traffic generation tools. An all-reduce operation logically consists of two phases: a reduce-scatter phase, where data is aggregated from all nodes to all nodes, and an all-gather phase, where the final aggregated result is distributed back to all nodes.

In a simulator like NS-3, you can implement this by developing a custom ns3::Application. This application would manage the multi-stage communication process required to mimic a ring-based or tree-based all-reduce algorithm. For instance, in a ring-based all-reduce:

Reduce-Scatter Phase: The application on each node would send a chunk of its data to its clockwise neighbor. It would then receive a chunk from its counter-clockwise neighbor, add it to its own data, and repeat this process N−1 times, where N is the number of nodes.

All-Gather Phase: Following the reduction, each node holds a piece of the final result. The nodes then perform another N−1 rounds of circular shifts to distribute their piece of the result to all other nodes.

This synthetic approach allows for highly controlled experiments where parameters like message size and the number of participating nodes can be easily varied. However, its primary drawback is a lack of realism. Synthetic models often fail to capture the subtle timing variations, the impact of computation time between communication phases, and the complex overlap of network transfers and GPU kernel execution that characterize your real-world applications.

3.2 High-Fidelity Trace-Based Workload Generation with ATLAHS
A more advanced and accurate method is to use trace-based workload generation. This approach involves capturing the communication and computation patterns of a real application running on a physical cluster and replaying them in your simulator. The ATLAHS (Application-centric Network Simulator Toolchain for AI, HPC, and Distributed Storage) project provides a comprehensive, open-source toolchain for this purpose. ATLAHS offers a significant leap in simulation fidelity by accurately reproducing the behavior of your complex, real-world applications.

The ATLAHS workflow consists of three main stages :

Trace Collection: Your first step is to profile a real application to capture its operational trace. ATLAHS provides tools for different application domains. For traditional HPC applications that use the Message Passing Interface (MPI), the liballprof library can be used to intercept PMPI calls and record all MPI operations and their timings. For modern AI training workloads that use NVIDIA's Collective Communications Library (NCCL), NVIDIA's Nsight Systems profiler is used to capture GPU stream activity, including NCCL operations and the execution of CUDA kernels.

GOAL Format Generation: The raw traces are then post-processed into a standardized format called GOAL (Graph of Operations Abstraction Layer). GOAL represents the entire distributed application as a Directed Acyclic Graph (DAG), where vertices represent fundamental operations—

send, recv for communication, and calc for computation—and edges represent dependencies between them. During this conversion, high-level collective operations like MPI_Allreduce or ncclAllReduce are decomposed into the specific sequence of point-to-point send and receive operations that the underlying library would execute, based on the library's configuration (e.g., ring or tree algorithm).

Simulator Integration: The final stage is to drive the network simulation using the generated GOAL file. ATLAHS provides a unified interface that abstracts away simulator-specific details. This interface reads the GOAL DAG and schedules the send, recv, and calc operations in the simulator at the appropriate times and on the appropriate nodes. For a send or recv operation, it triggers a network transfer. For a calc operation, it simply advances the simulation clock for that node by the specified computation duration, emulating the time the GPU or CPU would be busy. This allows ATLAHS to support multiple simulation backends, including NS-3 and OMNeT++.

3.3 The Necessity of Trace-Based Simulation for Security Analysis
For your security research, particularly in multi-tenant environments where workloads compete for resources, the realism provided by trace-based workloads isn't merely an incremental improvement but a fundamental requirement. Security attacks that manipulate congestion or flow control mechanisms have their most profound impact when the network is operating at or near its capacity limits. The synchronized, bursty traffic patterns characteristic of collective communications are precisely what creates these conditions of high contention and transient congestion, especially in common data center topologies like fat-trees.

A simple synthetic traffic model, such as one based on Poisson arrival processes, generates smooth, statistically distributed flows that fail to capture the "incast" scenarios and network hotspots created when dozens or hundreds of your nodes attempt to communicate simultaneously during an all-reduce step. This failure to model realistic congestion means that the simulated impact of a security attack will be artificially low. An attack that might cripple a real application by exacerbating an existing hotspot might show only a minor effect in a simulation with uniform random traffic.

By capturing the exact sequence, timing, and dependencies of communication and computation from a real application, ATLAHS reproduces the precise traffic dynamics that lead to realistic congestion scenarios. This provides a high-fidelity baseline against which the impact of a security attack can be accurately measured. To understand how a malicious actor can worsen the performance of a legitimate, high-performance application, your simulation must first be able to accurately model that application's baseline behavior under stress. Only a trace-based approach like that enabled by ATLAHS can provide this necessary level of fidelity, ensuring that the results of the security analysis are both meaningful and relevant to real-world systems.

Section 4: A Practical Guide to Simulating Network Security Attacks in RDMA Fabrics
This section provides concrete methodologies for implementing specific network security attacks within the most suitable simulation frameworks. The focus is on translating the theoretical principles of each attack into a practical simulation experiment, enabling you to model and quantify their impact on RoCEv2 and P4-based networks.

4.1 Exploiting Link-Layer Flow Control: Simulating PFC Pause Frame Storms
A Priority-based Flow Control (PFC) pause frame storm is a Layer 2 denial-of-service attack. A malicious or misconfigured node floods an upstream switch port with PFC pause frames for a specific traffic priority. This forces the switch to halt all transmissions for that priority class, effectively blocking not only the traffic destined for the malicious node but all other flows sharing that priority, leading to severe Head-of-Line (HoL) blocking across the network.

4.1.1 Simulation Methodology in NS-3
NS-3, equipped with the ns3-rdma module, is your ideal environment for this simulation due to its explicit implementation of the PFC mechanism.

Framework and Model Selection: Use an NS-3 simulation script with the ns3-rdma module enabled. Construct a topology, such as a simple dumbbell or a multi-stage fat-tree, that includes a shared bottleneck link where you can observe the attack's impact.

Malicious Node Implementation: The core of your attack lies in creating a malicious node that violates the standard PFC protocol. Instead of sending pause frames only when its own receive buffers are full, it will send them unconditionally and continuously. This can be implemented by creating a custom ns3::Application that runs on the attacker node.

Custom Frame Generation and Transmission: Your malicious application needs to craft and transmit raw MAC control frames. In NS-3, this involves several steps:

Create a new ns3::Packet object.

Define a custom ns3::Header that represents the exact byte format of an IEEE 802.3x pause frame, including the specific MAC Control opcode and the desired pause quanta for each of the eight priority classes. You would set a maximum pause time for the target priority and zero for all others.

Add this custom header to the packet using packet->AddHeader().

Your application must obtain a pointer to the node's NetDevice and use its Send() method to transmit this specially crafted packet to the MAC destination address of the connected switch port.

Attack Logic and Scheduling: To create a "storm," your application must send these pause frames repeatedly. This is achieved using the NS-3 simulator's event scheduler. Upon sending one pause frame, your application would use Simulator::Schedule() to schedule the transmission of the next frame after a very short delay, ensuring the upstream switch remains in a paused state for the duration of your attack.

4.2 Manipulating Congestion Signals: Simulating ECN Non-Compliance
This attack targets the cooperative nature of the DCQCN congestion control algorithm. A compliant endpoint is expected to reduce its transmission rate upon receiving feedback (CNPs) that its packets were ECN-marked. A malicious or "greedy" tenant can gain an unfair advantage by modifying its endpoint to ignore these congestion signals, maintaining a high sending rate while compliant flows are forced to back off. This starves legitimate traffic and can lead to severe buffer pressure, potentially triggering PFC storms and network-wide stalls.

4.2.1 Simulation Methodology in NS-3
The ns3-rdma module's detailed implementation of DCQCN makes NS-3 the definitive platform for modeling this attack. The attack requires modifying the simulator's source code.

Identify the Target Code: The attack is implemented by altering the logic of the DCQCN Reaction Point (RP), which is responsible for adjusting the sender's rate. You must locate the C++ source files within the ns3-rdma module that implement this sender-side behavior. The code to be modified will be the function or block that processes incoming Congestion Notification Packets (CNPs) and subsequently reduces the currentRate variable.

Implement the Malicious Behavior: Create a new subclass of the existing DCQCN implementation or add a configurable parameter (an ns3::Attribute) to the existing class to enable "malicious mode." Within this mode, the logic for rate reduction upon CNP receipt is bypassed. For example:

C++

void Dcqcn::OnCnpReceived(...)
{
  if (m_isMalicious)
  {
    // Ignore the CNP and do not reduce the rate.
    NS_LOG_INFO("Malicious node ignoring CNP.");
    return;
  }
  // Original rate reduction logic here...
  ReduceRate();
}
This modification ensures that your malicious node's congestion control algorithm never enters the rate reduction phase, effectively ignoring all ECN-based feedback.

Simulation Scenario: In your simulation script, configure a multi-tenant scenario where a "malicious" node (with the modified DCQCN) and several "compliant" victim nodes share a common bottleneck link. The malicious behavior can be enabled for the specific attacker node using the NS-3 attribute system.

4.3 Subverting Network Observability: Simulating Telemetry Leakage
In-band Network Telemetry (INT) provides unprecedented network visibility by embedding fine-grained state information (e.g., queue occupancy, latency, switch ID) directly into data packets. This attack involves a compromised programmable switch that illicitly copies packets containing this sensitive telemetry data and forwards them to an unauthorized external collector. This leakage can reveal critical information about network hotspots, path selection, and performance, which can be used to plan more sophisticated attacks.

4.3.1 Simulation Methodology in Mininet with P4
This data-plane-centric attack is best simulated using the P4 language in a Mininet-emulated environment with the BMv2 (Behavioral Model v2) software switch.

Framework and Topology: Set up a Mininet topology that includes source and destination hosts, one or more P4-enabled BMv2 switches, and an additional host designated as your "attacker collector," which isn't part of the legitimate traffic path.

Malicious P4 Program: The core of your attack is a custom P4 program loaded onto one of the switches. This program will implement the logic for both legitimate INT processing and malicious data exfiltration.

INT Processing: The P4 code will include headers and actions to add standard INT metadata (e.g., hop_latency, q_occupancy) to passing packets, as a normal INT-capable switch would.

Cloning and Exfiltration: The key malicious action uses the clone_ingress_pkt_to_egress primitive available in the v1model architecture. After the INT metadata has been added, your P4 ingress pipeline will execute an action that clones the packet.

Packet Modification: For the cloned packet, your P4 code will modify its destination L2/L3 addresses to match those of your attacker collector. It will also set the egress_spec to the port connected to the collector. The original packet is left unmodified and is forwarded normally.

Control Plane Configuration: You'll use a Python-based control plane script using the P4Runtime API to install the necessary rules into the switch's match-action tables. This includes rules to identify the target flow, apply the INT and cloning actions, and forward the packets appropriately.

4.4 Advanced Performance Hacking: Simulating QP-level Attacks
Academic research has shown that because RDMA congestion control is enforced on a per-Queue Pair (QP) basis, and because each new QP is allowed to start at line rate (an optimization for short flows), this mechanism can be exploited.

4.4.1 Simulation Methodology in NS-3
These attacks are implemented at the application layer, making NS-3 with ns3-rdma the appropriate simulation tool.

Custom Application Logic: You need a custom ns3::Application to orchestrate the attack. This application will be responsible for creating, managing, and sending data across multiple QPs (represented by ns3::Socket objects or an equivalent abstraction in the RDMA model).

Staggered QP Attack:

Your application creates a pool of QPs connected to the same destination.

It begins sending a burst of data on your first QP.

Using Simulator::Schedule(), it schedules an event to stop sending on the first QP and start sending on the second QP after a short duration—a period carefully chosen to be less than the time it would take for DCQCN to significantly throttle the first QP's rate.

This process is repeated, cycling through the pool of QPs. Each QP is used for a brief burst, always benefiting from the high initial sending rate, effectively circumventing the congestion control mechanism entirely.

Parallel QP Attack:

This attack is simpler to implement. Your application creates multiple QPs to the same destination.

Instead of cycling between them, it divides the total data to be sent among all QPs and transmits on all of them simultaneously.

Since the network's fair-sharing mechanism allocates bandwidth on a per-QP basis, your attacker's application unfairly obtains a share of the bottleneck bandwidth proportional to the number of QPs it opens, starving compliant applications that use a single QP.

Section 5: Quantifying the Impact: Metrics for Security Analysis in Multi-Tenant GPU Clusters
Simulating an attack is only your first step; the critical next step is to quantify its impact using metrics that are meaningful in the context of HPC and AI workloads. The consequences of a network attack in these environments are not just about packet loss or latency but translate directly into wasted computational resources, extended job runtimes, and violations of fairness between competing users.

5.1 Application and System Performance Degradation
The most direct measures of an attack's impact are those that capture the degradation of application performance and the inefficient use of expensive hardware resources like GPUs.

5.1.1 Job Completion Time (JCT)
Job Completion Time (JCT) is the single most important application-level performance metric. It represents the total wall-clock time from the initiation of your distributed computational job to its final completion. For communication-intensive workloads like distributed training, network performance is a primary determinant of JCT.

Measurement Methodology:

In NS-3: You must measure JCT within the custom application model that generates the workload (e.g., the synthetic all-reduce application or the ATLAHS trace-driven application). Your application should record the simulation time at the start of the first communication operation of a job using Simulator::Now(). Your application must then track the state of the distributed job. When the logic determines that the final required packet has been successfully received (e.g., the final all-gather phase is complete), it again calls Simulator::Now(). The difference between these two timestamps is the JCT for that job. The 

ns3::Application class provides StartTime and StopTime attributes, but these typically define the application's activity window and are not suitable for measuring the completion time of dynamic, network-dependent jobs.

In OMNeT++: A similar approach is taken. A custom simple module representing the application would record the simulation time (simTime()) at the start of a job. It would then process incoming messages and, upon receiving the final message that signifies job completion, record the end time. The difference yields the JCT. OMNeT++'s signal and statistic recording mechanisms can be used to automatically collect and process these JCT values.

5.1.2 GPU Utilization (Modeling Approach)
Network simulators like NS-3 and OMNeT++ don't inherently model the internal architecture or execution of GPUs. However, the impact of network performance on GPU utilization is a critical factor that can and should be modeled. In distributed training, GPUs frequently stall, waiting for data to arrive over the network before they can begin the next computation phase. An effective network attack increases this communication time, directly leading to lower GPU utilization and wasted computational cycles.

Co-Simulation Modeling Methodology:

Baseline Application Profiling: First, profile your real-world application (e.g., using NVIDIA's Nsight) to establish a baseline. This profile provides the intrinsic computation time for various kernels and the data sizes for communication operations. This step is implicitly handled if using a trace-based workload generator like ATLAHS, as the trace contains both computation (calc) and communication (send/recv) events.

Simulate Communication Phases: Run your application's workload trace in the network simulator (e.g., NS-3) under two conditions: a baseline (no attack) and with your security attack active. The simulator will output the total time spent in communication for the entire job under both scenarios.

Calculate Effective GPU Utilization: Use a post-processing script to calculate your effective GPU utilization. Your total job time is the sum of the total intrinsic computation time (from the trace) and the total simulated communication time. The formula is:

Effective GPU Utilization= 
∑T 
compute
​
 +T 
communication_simulated
​
 
∑T 
compute
​
 
​
 
By comparing the utilization percentage calculated with the baseline communication time versus the attack communication time, you can directly quantify the attack's impact on the efficiency of expensive GPU resources. For example, if an attack doubles the communication time, your job that was previously 50% communication-bound (and thus 50% GPU utilized) would become 66% communication-bound, dropping its GPU utilization to 33%. This gives you a powerful and intuitive metric for expressing the financial and operational cost of a network security vulnerability.

5.2 Breaches in Isolation and Fairness
In multi-tenant HPC or cloud environments, where multiple users or jobs share the same network fabric, ensuring performance isolation is paramount. A security attack shouldn't allow one malicious tenant to monopolize network resources at the expense of other, well-behaved tenants.

Scenario Setup: To measure fairness, your simulation topology must be configured to model a shared resource. A common approach is a dumbbell topology with a central bottleneck link shared by multiple source-destination pairs. In your simulation, one pair of nodes is designated as the "attacker" (e.g., running a non-compliant DCQCN), while other pairs are "victims" (running the standard, compliant protocol).

Measurement Metrics:

Throughput Analysis: The most direct measure of unfairness is the bandwidth allocation at the bottleneck link. Using your simulator's flow monitoring or tracing capabilities, measure the goodput (application-level throughput) achieved by the attacker's flow(s) and the victim's flow(s). In a successful attack, the attacker's throughput will be significantly higher than its fair share, while the victims' throughput will be correspondingly starved.

JCT Degradation for Victims: Measure the Job Completion Time for a standard task run by your victim tenant under two conditions: (1) running in isolation, and (2) running concurrently with the attacker tenant's malicious workload. The percentage increase in your victim's JCT provides a clear and compelling measure of the collateral damage caused by the attack's breach of performance isolation.

Jain's Fairness Index: To produce a single, quantitative score for fairness across your multiple tenants, Jain's Fairness Index can be calculated from the throughputs (x 
i
​
 ) achieved by each of the n tenants:

J(x 
1
​
 ,x 
2
​
 ,…,x 
n
​
 )= 
n∑ 
i=1
n
​
 x 
i
2
​
 
(∑ 
i=1
n
​
 x 
i
​
 ) 
2
 
​
 
The index ranges from 1/n (worst case) to 1 (perfect fairness). Comparing the fairness index of your baseline scenario to your attack scenario provides a standardized measure of the attack's impact on resource allocation equity.

Section 6: Synthesis and Strategic Recommendations for Future Research
This analysis has surveyed the landscape of simulation and emulation frameworks for studying the security of high-performance RoCEv2 and InfiniBand networks. It's provided detailed methodologies for modeling realistic AI/HPC workloads, implementing a range of sophisticated network attacks, and quantifying their impact on application performance and system fairness. The findings culminate in a set of strategic recommendations for researchers in this domain, highlighting the optimal tools for specific research goals and identifying critical areas for future investigation.

6.1 A Decision Framework for Researchers
The choice of a simulation or emulation tool is not a one-size-fits-all decision; you must tailor it to your specific research objective. Based on the detailed evaluation in Section 2, the following decision framework is recommended:

For Prototyping Novel Protocol-Level Attacks: When your research goal is to discover new vulnerabilities in protocol logic or to design new attacks that exploit specific behaviors (e.g., ECN non-compliance, QP-level performance hacking), NS-3 with the ns3-rdma module is the unequivocal choice. Its open-source nature, detailed C++ models of DCQCN and PFC, and high degree of extensibility provide you the necessary environment for modifying and instrumenting protocol internals.

For Research on Programmable Data Planes and Telemetry: For any investigation you're conducting involving P4, In-band Network Telemetry (INT), or custom data plane logic (e.g., the telemetry leakage attack), Mininet integrated with the BMv2 software switch is the industry and academic standard. It provides you the perfect blend of realistic network emulation and high-fidelity simulation of programmable switch behavior.

For Validating Configurations and System-Level Impact on Vendor Stacks: When your objective is to understand how a known attack or a complex configuration affects a production-like software stack, a high-fidelity emulation platform is required. NVIDIA Air serves this role by providing a digital twin of NVIDIA's networking ecosystem, allowing you realistic validation of system behavior without access to physical hardware.

For Rapid, Focused Analysis of Congestion Control Algorithms: If your research is narrowly focused on the performance dynamics and stability of different congestion control algorithms, Broadcom's htsim offers a significant advantage in simulation speed. Its optimization for this specific task allows you much faster exploration of large parameter spaces compared to general-purpose simulators.

6.2 Key Findings on RDMA Security Posture
This report's illuminated several critical aspects of the security posture of modern RDMA fabrics, particularly RoCEv2:

The Fragility of Cooperation: RoCEv2's high performance is built upon a foundation of cooperative, multi-layer control mechanisms (PFC and ECN/DCQCN). This analysis reaffirms these mechanisms are fragile. Their reliance on all your network participants adhering strictly to the protocol specification makes them vulnerable to malicious or misconfigured endpoints that can trigger cascading failures, such as HoL blocking or bandwidth starvation.

The Short Flow Optimization Loophole: The design choice to allow new RDMA Queue Pairs to start at line rate—an optimization intended to minimize latency for short messages—creates a significant security loophole. As demonstrated by the Parallel and Staggered QP attacks, malicious applications can exploit this feature to completely bypass the intended congestion control mechanism, gaining a massively disproportionate share of network bandwidth.

Telemetry as a New Attack Vector: The rise of programmable data planes and INT introduces a new dimension to network security. While providing invaluable visibility, telemetry mechanisms can also become vectors for information leakage. Compromised switches can exfiltrate sensitive network state, providing adversaries with the intelligence they need to launch more targeted and effective attacks.

6.3 Future Research Directions
The methodologies and findings presented in this report also point toward several promising and critical areas for your future research:

High-Fidelity Network-GPU Co-Simulation: A significant gap exists in the current tooling. The impact of network performance on GPU utilization is currently modeled via post-processing of your simulation results. A true co-simulation framework that tightly integrates a packet-level network simulator (like NS-3) with a cycle-accurate GPU simulator (like Accel-sim ) is a critical next step. Such a framework would enable you to study dynamic feedback loops where network congestion could influence GPU scheduling decisions in real time, and vice-versa, providing you a far more accurate picture of system performance under stress and attack.

Hardware-Based Defenses and Enforcement: You need research into hardware-level defenses that can be modeled in simulation. For instance, could a programmable NIC or switch ASIC be designed to detect and rate-limit the rapid creation and cycling of QPs characteristic of performance hacking attacks? Simulating such hardware logic in a framework like NS-3 or a P4-based model could prove these defenses' viability before physical implementation.

Secure and Authenticated Telemetry: The telemetry leakage attack highlights the need for secure network observability. Your future research should focus on designing and simulating lightweight cryptographic mechanisms to protect INT data. This could involve using P4 to model the addition of message authentication codes (MACs) to telemetry reports, ensuring their integrity and authenticating their origin, thereby preventing both tampering and unauthorized exfiltration.
